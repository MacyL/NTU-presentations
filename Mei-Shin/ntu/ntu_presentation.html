<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>reveal-md</title>
    <link rel="stylesheet" href="./css/reveal.css" />
    <link rel="stylesheet" href="./css/theme/simple.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./css/print/paper.css" type="text/css" media="print" />
    <link rel="stylesheet" href="./_assets/additional.css" />
    <link rel="stylesheet" href="./_assets/spreadsheet.css" />

  </head>
  <body>
    <div class="navi">
        <p id='intro'>INTRODUCTION</p>
        <p id='methods'>METHODS</p>
        <p id='workflows'>WORKFLOWS</p>
        <p id='modeling'>MODELING</p>
        <p id='outlook'>OUTLOOK</p>
    </div>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template"><h4>Fundamentals of computer-assisted language comparison</h4>
<p><img src='img/calc-yinyang.png' style="border: 0.1;border-color: white;width:25%"></img></p>

<p style='font-size:30px; text-align:center;'>International Conference on Historical Linguistics #24</p>
<p style='font-size:20px; text-align:center;'>2019.07.03</p></script></section><section ><section data-markdown><script type="text/template"><div class='intro'></div>

<h3>Computer-based Linguistics<h3>

<p>Tiago Tresoldi</p></script></section><section data-markdown><script type="text/template"><img src='http://lingulist.de/documents/talks/img/calc-project/calc-5.jpg' style='border:0px; width:80%'></img></script></section><section data-markdown><script type="text/template">
<img src='http://lingulist.de/documents/talks/img/calc-project/calc-7.jpg' style='border:0px; width:80%'></img></script></section></section><section ><section data-markdown><script type="text/template"><div class='methods'></div>

<h3>Methods and algorithms<h3>

<p>Tiago Tresoldi</p>
</script></section><section data-markdown><script type="text/template"><div class="fig-container" style="overflow:hidden;"
        data-overflow-shown=true
        data-file="http://lingpy.org/"
        data-style="height: 600px; margin-top: -100px; width: 100%;">
</div>
</script></section></section><section ><section data-markdown><script type="text/template"><div class='workflows'></div>
<h3>CALC workflows</h3>
<p>Mei-Shin Wu</p></script></section><section data-markdown><script type="text/template"><p style='text-font:25px'>The Gap Between Computational and Traditional Historical Linguistics</p>
<div class="fig-container" style="overflow:hidden;"
        data-overflow-shown=true
        data-file="http://lingulist.de/documents/talks/img/ba-talk/background-11.jpg"
        data-style="height: 550px; margin-top: -100px; width: 100%;">
</div>
<aside class="notes"><p>The comparative method has been applied to many other language
families.</p>
</aside></script></section><section data-markdown><script type="text/template"><p style='text-font:25px'>The Gap Between Computational and Traditional Historical Linguistics</p>
<div class="fig-container" style="overflow:hidden;"
        data-overflow-shown=true
        data-file="http://lingulist.de/documents/talks/img/ba-talk/background-12.jpg"
        data-style="height: 550px; margin-top: -100px; width: 100%;">
</div>
<aside class="notes"><p>But our knowledge of the history of most of these
language families is still rather fuzzy. Especially, some language families in South East Asia, like Sino-Tibetan, Hmong-Mien and Tai-Kadai language families. As language data accumulated through time, comparative methods have reached its practical limits. In the situation where linguists cannot digest the large amount of data, but the computer approach cannot yield high accuracy results, it is time to consider a new framework.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>A computer-assisted approach</h3>

<p>To allow humans and machines to work together successfully, it is important that:</p>
<ul style='text-align:left;'>
    <li> our data is both human- and machine-readable, </li>
    <li> we follow transparent guidelines when handling linguistic datasets, </li>
    <li>we offer interfaces that allow humans and machines to access the data at the same time. </li>
</ul>

<aside class="notes"><p>A computer-assisted strategy has been integrated in many scientific fields. We adapted the idea and develop the computer-assisted framework in historical linguistics. The basic idea behind <b>computer-assisted</b> as opposed to <b>computer-based</b> language comparison is to allow scholars to do qualitative and quantitative research at the same time. By combining the efforts from experts and computing power, we can get the best of two worlds, the efficiency of computers, and the accuracy of humans.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>CALC workflow</h3>
<img src='img/HillList.png'></img>

<aside class="notes"><p>Our workflows for computer-assisted language
comparison have so far been intensively tested on a
small set of 8 Burmish languages, which we
investigated in collaboration with Nathan Hill, who
was responsible for the qualitative investigation of
the data and for the common discussion of new
computer-assisted methods which were then
implemented by Mattis List.</p>
<p>Our experience with this Burmish project allows us to
set up this workflow that starts from raw data to the
explicit identification of correspondence patterns
across multiple languages. At the moment, List and
Hill develop the workflow further to account also for
(semi)-automatic reconstructions, but in this talk, only
the identification of correspondence patterns will be
discussed.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>Details of the workflows</h3>
<p style="text-align:center">
<img src="img/calc-workflow.png" alt="img" style='border:0px'></img>
</p>

<aside class="notes"><p>This picture presents the full workflow, it comprises 5
different processes at this moment, in which we
successively lift linguistic data from their raw form
up to a level where correspondence patterns across
cognate words have been automatically identified and
can be qualitatively inspected by the scholars.
Some technical terms on this picture may look
unfamiliar to you, but the ideas behind these
applications are actually being practiced by linguists
for quite some time already. In the following, we will
discuss these ideas in detail, and you find even more
detailed information in the handout accompanying
this talk.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='30px'>Materials and methods</p>

<img src='img/languages.png' style='border:0px;width:60%;'></img>

<ul >
<li style='font-size:60%'> Chén 陳其光 (2012). Miao and Yao language. 苗瑤语文</li>
<li style='font-size:60%'> 25 Hmong-Mien languages in the original (10 in our selection)</li>
<li style='font-size:60%'> 885 concepts in the original (313 in our selection, compatible with the Burmish Etymological dictionary project)</li>
</ul>

<aside class="notes"><p>The data we use to illustrate our workflow was
originally collected by 陳其光, and later added in
digital form to the Wiktionary project.
Chén&#39;s collection of <b>frequent terms</b> comprises 885
different concepts translated into 25 varieties of
Hmong-Mien. In this talk, we extract 10 Hmong-Mien
languages for the demonstration, and the map here
presents the geographical locations of these
languages.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>From raw data to machine-readable data</h3>
<p style='text-align:center'>
<img src="img/raw-machine.png" style="width:700px;border:0px" alt="img"></img>
</p>

<aside class="notes"><p>The first step is to convert raw data to a machine
readable format. I will try to explain in detail, what
this means.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='30px'>From raw data to machine-readable data</p>
<img src="img/chen-illustration.png" style="width:800px;border:0px" alt="img"></img>

<aside class="notes"><p>To see in detail, what this means, let’s have a look at
one exemplary page from Chén’s book, with the data,
as it has been prepared by the SEALANG project.
We can see that the data is essentially the same, but
that the rows and columns of the tabular form have
been swapped.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='30px'>From raw data to machine-readable data</p>

<div class="spreadsheet" data-delimiter="\t" data-width="100" fontsize="12">
     \t Baheng,east \t Baheng, west \t Qiandong, east \t Qiandong, wesst
七   \t  tsha³¹,tsjung⁴⁴ \t tshang⁴⁴    \t     shung⁵³    \t      shung²²
月亮 \t la⁰³lha⁵⁵ \t ʔa⁰³lha⁵⁵ \t la⁴⁴la⁴⁴ \t pau¹¹la³³
星星 \t la⁰³qang³⁵ \t qa⁰³qang³⁵ \t qei²⁴qei²⁴ \t tei⁴⁴qei⁴⁴
</div>

<aside class="notes"><p>The problem of this type of data is that it is difficult to
interpret for a computer. This is because it contradicts
one fundamental principle of data organization: one
cell in a table should have only one kind of value.
But in the tables in Chen’s data, we can often see
multiple values in a cell, and often they are there to
indicate that a language has two or more expressions
for the same concept. They then separate these
synonyms by some character, a comma, a colon, a dot,
or a pipe. <b>(action)</b> This may look okay for humans, but
it will confuse any computer method, as the method
cannot guess what the human wants to say here.</p>
<p>(action is to type , ; and | in the cell)</p>
</aside></script></section><section data-markdown><script type="text/template"><div class="spreadsheet" data-delimiter="\t" data-width="60">
 ID \t DOCULECT        \t CONCEPT \t ENGLISH \t VALUE         \t FORM \t TOKENS \t NOTE
 1  \t Baheng, east    \t 七      \t SEVEN   \t tsja³¹,tsjung⁴⁴ \t tsja³¹     \t        \t
 2  \t Baheng, east    \t 七      \t SEVEN   \t tsja³¹,tsjung⁴⁴ \t tsjung⁴⁴    \t        \t variant
 2  \t Baheng, west    \t 七      \t SEVEN   \t tsjang⁴⁴      \t tsjang⁴⁴      \t        \t
 3  \t Qiandong, east  \t 七      \t SEVEN   \t sjung⁵³       \t sjung⁵³       \t        \t
 4  \t Qiandong, wesst \t 七      \t SEVEN   \t sjung²²       \t sjung²²       \t        \t
 5  \t Baheng, east    \t 月亮    \t MOON    \t la⁰³lha⁵⁵     \t la⁰³lha⁵⁵     \t        \t
 6  \t Baheng, west    \t 月亮    \t MOON    \t ʔa⁰³lha⁵⁵     \t ʔa⁰³lha⁵⁵     \t        \t
 7  \t Qiandong, east  \t 月亮    \t MOON    \t la⁴⁴la⁴⁴      \t la⁴⁴la⁴⁴      \t        \t
 8  \t Qiandong, wesst \t 月亮    \t MOON    \t pau¹¹la³³     \t pau¹¹la³³     \t        \t
 9  \t Baheng, east    \t 星星    \t STAR    \t la⁰³qang³⁵    \t la⁰³qang³⁵    \t        \t
 10 \t Baheng, west    \t 星星    \t STAR    \t qa⁰³qang³⁵    \t qa⁰³qang³⁵    \t        \t
 11 \t Qiandong, east  \t 星星    \t STAR    \t qei²⁴qei²⁴    \t qei²⁴qei²⁴    \t        \t
 12 \t Qiandong, wesst \t 星星    \t STAR    \t tei⁴⁴qei⁴⁴    \t tei⁴⁴qei⁴⁴    \t        \t
</div>

<aside class="notes"><p>We transform the wide format to a so-called longtable
format, which looks redundant at first sight, but
is the most easy-to-make way to provide data that is
machine-readable.
Each entry in this format consists of a unique id, a
language name (Doculect), a concept identifier (if it’s
not English then you can translate it into English), and
a value. The value is the original entry we find in the
source. This value is then further split, if it contains a
comma, and we add the other entry to the FORM
column. In this way, we can consistently handle
synonyms, but also keep track of the original data.</p>
<p>Now, the form is not yet computer-readable. Linguists
would not simply compare the word forms, but
computers don’t know what one sound is, and how the
sounds should be interpreted.
For this reason, we have to segment the data for the
computer, so the methods know which symbols form
one sound. We do this by adding spaces.
The most straightforward way is to segment by hand.
(action) But if we are dealing with hundreds of entries, it is
better to do this automatically.
(action is to type : )
tsja³¹ → tɕ (U+0255)a ³(U+00b3)¹(U+00b9) → tɕ a ³¹</p>
<p>⁵ (U+2075)
⁴ (U+2074)</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='30px'>From raw data to machine-readable data</p>

We recommend <i>Orthography Profiles</i> as a way to:

<ul>
<li> Convert arbitrary input data to IPA: </li>
  <ul style="list-style-type:none;">
    <li> tsj   ---->  tɕ </li>
    <li> ng    ---->   ŋ </li>
  </ul>
<li>And to segment the input data:</li>
   <ul style="list-style-type:none;">
      <li> tsja³¹  ----> tɕa³¹ ----> tɕ  a ³¹<li>
   </ul>
</ul>

<aside class="notes"><p>We recommend to use Orthography Profiles to
convert all kinds of transcriptions to consistent IPA
and segment the data at the same time.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='30px'>From raw data to machine-readable data</p>

<div class="spreadsheet" data-delimiter="\t" data-width="80">
Grapheme \t IPA
č        \t tʃ
ž        \t dʒ
th       \t tʰ
dh       \t d̤
sh       \t ʃ
a        \t a
aa       \t aː
tsj	 \t tɕ
la	 \t l a
</div>

<aside class="notes"><p>An orthography profile is nothing else than a table, in
which you list the combination of characters in the
original transcription in the first column, and how it
should be converted in a second column.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='30px'>From raw data to machine-readable data</p>
<div class="spreadsheet" data-delimiter="\t" data-width="100" data-fontsize="13">
ID  \t DOCULECT        \t CONCEPT \t ENGLISH \t VALUE           \t FORM       \t TOKENS              \t COGIDS
 1  \t Baheng, east    \t 七      \t SEVEN   \t tsja³¹,tsjung⁴⁴ \t tsja³¹     \t tɕ a ³¹             \t
 2  \t Baheng, east    \t 七      \t SEVEN   \t tsja³¹,tsjung⁴⁴ \t tsjung⁴⁴   \t tɕ u ŋ ⁴⁴           \t
 3  \t Baheng, west    \t 七      \t SEVEN   \t tsjang⁴⁴        \t tsjang⁴⁴   \t tɕ a ŋ ⁴⁴           \t
 4  \t Qiandong, east  \t 七      \t SEVEN   \t sjung⁵³         \t sjung⁵³    \t ɕ u ŋ ⁵³            \t
 5  \t Qiandong, wesst \t 七      \t SEVEN   \t sjung²²         \t sjung²²    \t ɕ u ŋ ²²            \t
 6  \t Baheng, east    \t 月亮    \t MOON    \t la⁰³lha⁵⁵       \t la⁰³lha⁵⁵  \t l a ³/⁰ + ɬ a ⁵⁵    \t
 7  \t Baheng, west    \t 月亮    \t MOON    \t ʔa⁰³lha⁵⁵       \t ʔa⁰³lha⁵⁵  \t ʔ a ³/⁰ + ɬ a ⁵⁵    \t
 8  \t Qiandong, east  \t 月亮    \t MOON    \t la⁴⁴la⁴⁴        \t la⁴⁴la⁴⁴   \t l a ⁴⁴ + l a ⁴⁴     \t
 9  \t Qiandong, wesst \t 月亮    \t MOON    \t pau¹¹la³³       \t pau¹¹la³³  \t p ɔ ¹¹ + l a ³³     \t
 10 \t Baheng, east    \t 星星    \t STAR    \t la⁰³qang³⁵      \t la⁰³qang³⁵ \t l a ³/⁰ + q a ŋ ³⁵  \t
 11 \t Baheng, west    \t 星星    \t STAR    \t qa⁰³qang³⁵      \t qa⁰³qang³⁵ \t q a ³/⁰ + q a ŋ ³⁵  \t
 12 \t Qiandong, east  \t 星星    \t STAR    \t qei²⁴qei²⁴      \t qei²⁴qei²⁴ \t q ei ²⁴ + q ei  ²⁴  \t
 13 \t Qiandong, wesst \t 星星    \t STAR    \t tei⁴⁴qei⁴⁴      \t tei⁴⁴qei⁴⁴ \t t ei - ⁴⁴ + q ei ⁴⁴ \t
</div>

<aside class="notes"><p>And once we applied the profile, our data looks like
this. Note that the plus-sign here indicates, that the
word consists of two morphemes.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>From segmented words to computer-inferred cognates</h3>
<p style='text-align:center'>
<img src="img/machine-partial.png" style="width:800px;border:0px" alt="img"></img>
</p>

<aside class="notes"><p>Now, we come to the second stage, in which we try to
infer partial cognates from our segmented words.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='30px'>From segmented words to computer-inferred cognates</p>

<p style='text-align:center'>
<img src="img/partialcg.png" style="width:800px" alt="img"></img>
</p>

<aside class="notes"><p>Compounding is an important element of word
formation in South-East Asian languages. The
presence of compound words challenges the notion
that words can either be cognate or not. This picture
shows an example of words for “moon” in 4 Sinitic
languages, the words that should be cognates are
marked in the same color. For example, ŋuoʔ5, ŋiat5,
ȵy21, yɛ51 are all cognate with each other. But
Meixian has another morpheme which means “light”
in Mandarin Chinese, and Wenzhou has two
morphemes kuɔ35 vai13 after the moon morpheme.
If we allow words only to be cognate or not, we
probably should say that we have four different
cognates here.
To account for partial cognates in our data, we use a
different annotation schema. In this schema, we assign
each morpheme a cognate ID, and if two morphemes
have the same ID, they are thus meant to be cognate.</p>
</aside></script></section><section data-markdown><script type="text/template"><p style='font-size:30px'>From segmented words to computer-inferred cognates</p>

<div class="fig-container" style="overflow:hidden;"
        data-overflow-shown=true
        data-file="http://lingulist.de/documents/talks/img/calc-project/algo2.jpg"
        data-style="height: 310px; margin-top: -20px; width: 80%;">
</div>

<p style='text-align:left;font-size:60%;'>List et al. (2016). Using sequence similarity networks to identify partial cognates in multilingual wordlists. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol. 2, pp. 599-605).</p>

<aside class="notes"><p>With the method by Mattis List et al., proposed in
2016, we have a rather simple and efficient approach
to automatically search for cognates in linguistic
datasets.
Although the algorithm is not very complex, it would
go too far to explain the details here, I am afraid, and
therefore, I will only show this nice graph, that
illustrates the different stages, and tell you that the
core idea is to model the data with help of networks.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='30px'>From segmented words to computer-inferred cognates</p>

<div class="spreadsheet" data-delimiter="\t" data-width="100" data-fontsize="13">
ID  \t DOCULECT        \t CONCEPT \t ENGLISH \t VALUE           \t FORM         \t TOKENS              \t COGIDS
 1  \t Baheng, east    \t 七      \t SEVEN   \t tsja³¹,tsjung⁴⁴ \t tsja³¹      \t tɕ a ³¹             \t 3
 2  \t Baheng, east    \t 七      \t SEVEN   \t tsja³¹,tsjung⁴⁴ \t tsjung⁴⁴    \t tɕ u ŋ ⁴⁴           \t 3
 3  \t Baheng, west    \t 七      \t SEVEN   \t tsjang⁴⁴        \t tsjang⁴⁴    \t tɕ a ŋ ⁴⁴           \t 3
 4  \t Qiandong, east  \t 七      \t SEVEN   \t sjung⁵³         \t sjung⁵³     \t ɕ u ŋ ⁵³            \t 3
 5  \t Qiandong, wesst \t 七      \t SEVEN   \t sjung²²         \t sjung²²     \t ɕ u ŋ ²²            \t 3
 6  \t Baheng, east    \t 月亮    \t MOON    \t la⁰³lha⁵⁵       \t la⁰³lha⁵⁵   \t l a ³/⁰ + ɬ a ⁵⁵    \t 1908 1907
 7  \t Baheng, west    \t 月亮    \t MOON    \t ʔa⁰³lha⁵⁵       \t ʔa⁰³lha⁵⁵   \t ʔ a ³/⁰ + ɬ a ⁵⁵    \t 1909 1907
 8  \t Qiandong, east  \t 月亮    \t MOON    \t la⁴⁴la⁴⁴        \t la⁴⁴la⁴⁴    \t l a ⁴⁴ + l a ⁴⁴     \t 1908 1907
 9  \t Qiandong, wesst \t 月亮    \t MOON    \t pau¹¹la³³       \t pau¹¹la³³   \t p ɔ ¹¹ + l a ³³     \t 1910 1907
 10 \t Baheng, east    \t 星星    \t STAR    \t la⁰³qang³⁵      \t la⁰³qang³⁵  \t l a ³/⁰ + q a ŋ ³⁵  \t 1874 1870
 11 \t Baheng, west    \t 星星    \t STAR    \t qa⁰³qang³⁵      \t qa⁰³qang³⁵  \t q a ³/⁰ + q a ŋ ³⁵  \t 　1872 1870
 12 \t Qiandong, east  \t 星星    \t STAR    \t qei²⁴qei²⁴      \t qei²⁴qei²⁴  \t q ei ²⁴ + q ei  ²⁴  \t 　1872 1870
 13 \t Qiandong, wesst \t 星星    \t STAR    \t tei⁴⁴qei⁴⁴      \t tei⁴⁴qei⁴⁴  \t t ei - ⁴⁴ + q ei ⁴⁴ \t 　1871 1870

</div>

<aside class="notes"><p>If we apply this method to our data, we get results that
look as follows. All words are given cognate IDs by
the algorithm, depending on how many morphemes
they have, and if the IDs are identical, this means the
algorithm judges the words to be cognate.</p>
</aside></script></section><section data-markdown><script type="text/template"><p style='font-size:30px;'>From cognates to alignments</p>

<p style='text-align:center'>
<img src="img/partial-alignment.png" style="border:0px;width:800px" alt="img"></img>
</p>

<aside class="notes"><p>In the third stage, we want to align the cognates we
detected.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='30px'>From cognates to alignments</p>
<p align="middle"><img src='img/alignment.png' style="width:400px;align:middle;border:0px"></img></p>

Phonetic alignment techniques are well-known in historical linguistics and have been applied for quite some time now.

<aside class="notes"><p>Phonetic alignment techniques are well-known in
historical linguistics and have been applied for quite
some time now. As the figure shows here, cognate
words (here all meaning “seven”) are arranged into a
matrix so that corresponding segments are placed in
the same column. This is essential to identify sound
correspondences. Nowadays, we have stable
algorithms for multiple alignments that yield accuracy
scores almost comparable to the differences we would
expect between human annotators only, and we also
have web-based tools that facilitate manual
alignments greatly. This picture, for example, is taken
from the EDICTOR application, and I will show you
how to work with this tool in the last section. But
even if this helps to save some time, it is still tedious
to correct alignments manually.</p>
</aside></script></section><section data-markdown><script type="text/template">
<h3>From cognates to alignments</h3>

<p style='font-size:30px;text-align:left;'>We propose <i>Template-Based Alignments</i> as an alternative to semi-automatically computed alignments.</p>

<ul>
  <li style='font-size:25px'> Languages with a rather restricted syllable structure can usually be aligned in a very consistent way by simply using a template. </li>
  <li style='font-size:25px'>   A typical Chinese syllable, for example, consists of <i>initial</i>, <i>medial</i>, <i>nucleus</i>, <i>coda</i> and <i>tone</i> (Wang 1996). Once we know the individual template of a Chinese word, we can easily align it with any other word, as long as we know the template.</li>
</ul>

<aside class="notes"><p>We propose Template-Based Alignments as an
alternative to semi-automatically computed
alignments.
The major idea is, that languages with a rather
restricted syllable structure can usually be aligned in a
very consistent way by simply using a template.
A typical Chinese syllable, for example, consists of
<em>initial</em>, <em>medial</em>, <em>nucleus</em>, <em>coda</em> and <em>tone</em>
(Wang 1996). Once we know the individual template
of a Chinese word, we can easily align it with any
other word, as long as we know the template.</p>
</aside></script></section><section data-markdown><script type="text/template">
<p stlye='fontsize:30px'>From cognates to alignments</p>
<p align="middle"><img src='img/templates.png' style="width:1000px;align:middle;border:0px"></img></p>

<aside class="notes"><p>Here is an example for this workflow, provided we
know the template for the words in our data. We start
from the tokens, and we use the Structure column to
provide information on the template. Now we use a
meta-template of the general syllable structure of the
languages, and then we drop all those columns, where
we do not find a sound.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='fontsize:30px'>From cognates to alignments</p>

<p align="middle"><img src='img/orthotemplate.png' style="width:600px;align:middle;"></img></p>
<aside class="notes"><p>The problem is of course, how to make the templates
for the words in our data? Here, we can again use
orthography profiles, along with a variant in which we
can provide rudimentary context, here expressed by
the circumflex symbol for the beginning of a word,
and the dollar sign for the end. We just add another
column, and in this column we provide the structure,
the template, for the given sub-sequence. You find
more information in the handout.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='fontsize:30px'>From cognates to alignments</p>

<div class="spreadsheet" data-delimiter="\t" data-width="100" data-fontsize="13">
ID  \t DOCULECT        \t ENGLISH \t TOKENS             \t STRUCTURE \t ALIGNMENT \t COGIDS
 1   \t Baheng, east   \t SEVEN   \t tɕ a ³¹             \t i n t     \t tɕ a - ³¹ \t 3
 2   \t Baheng, west   \t SEVEN   \t tɕ a ŋ ⁴⁴           \t i n c t   \t tɕ a ŋ ⁴⁴ \t 3
 3   \t Qiandong, east \t SEVEN   \t ɕ u ŋ ⁵³            \t i n c t   \t  ɕ u ŋ ⁵³ \t 3
 4   \t Qiandong, wesst\t SEVEN   \t ɕ u ŋ ²²            \t i n c t   \t ɕ u ŋ ²²  \t 3
 5   \t Baheng, east   \t MOON    \t l a ³/⁰ + ɬ a ⁵⁵    \t i n t + i n t \t l a ³/⁰ + ɬ a ⁵⁵\t 1908 1907
 6   \t Baheng, west   \t MOON    \t ʔ a ³/⁰ + ɬ a ⁵⁵    \t i n t + i n t \t ʔ a ³/⁰ + ɬ a ⁵⁵ \t 1909 1907
 7   \t Qiandong, east \t MOON    \t l a ⁴⁴ + l a ⁴⁴    \t i n t + i n t \t l a ⁴⁴ + l a ⁴⁴ \t 1908 1907
 8   \t Qiandong, wesst\t MOON    \t p ɔ ¹¹ + l a ³³    \t i n t + i n t \t p ɔ ¹¹ + l a ³³ \t 1910 1907
 9   \t Baheng, east   \t STAR    \t l a ³/⁰ + q a ŋ ³⁵ \t i n t + i n c t \t l a ³/⁰ + q a ŋ ³⁵ \t 1874 1870
 10  \t Baheng, west   \t STAR    \t q a ³/⁰ + q a ŋ ³⁵ \t　i n t + i n c t \t q a ³/⁰ + q a ŋ ³⁵ \t 1872 1870
 11  \t Qiandong, east \t STAR    \t q ei ²⁴ + q ei  ²⁴ \t　i n t + i n t \t q ei ²⁴ + q ei - ²⁴ \t  1872 1870
 12  \t Qiandong, wesst\t STAR    \t t ei - ⁴⁴ + q ei ⁴⁴\t　i n t + i n t \t t ei - ⁴⁴ + q ei - ⁴⁴\t 1871 1870
</div>

<aside class="notes"><p>The output file then has two more columns which is
called “Alignment” and “Structure”.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>From alignments to strict, cross-semantic cognates</h3>

<p style='text-align:center'>
<img src="img/alignment-strict.png" style="width:800px" alt="img"></img>
</p>

<aside class="notes"><p>We are almost done, we now need to infer the strict
cross-semantic cognates from the data.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='fontsize:30px'>From alignments to strict, cross-semantic cognates</p>

* For a realistic analysis, we need to identify cognates not only within the same meaning slot, but across different concepts.
* However, our algorithm for automatic congate detection designed to search words with the same meaning.
* Therefore, we need to find *cross-semantic* partial (=normal) cognates in a second stage.

<aside class="notes"><p>For a realistic analysis, we need to identify cognates
not only within the same meaning slot, but across
different concepts.
However, our algorithm for automatic congate
detection designed to search words with the same
meaning.
Therefore, we need to find cross-semantic partial
(=normal) cognates in a second stage.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='fontsize:30px'>From alignments to strict, cross-semantic cognates</p>

* For this task, we employ a new algorithm to <i>merge</i> cognates in our data into larger groups.
* The basic idea is to check if two alignments are compatible with each other, and to fuse them to form a bigger alignment, if this is the case.
* As a side effect, all words we identify in this way are <i>strictly</i> cognate, since our procedure does not allow to identify a morpheme in the same language to be cognate if this does not show the exact same form.

<aside class="notes"><p>For this task, we employ a new algorithm to merge
cognates in our data into larger groups.
The basic idea is to check if two alignments are
compatible with each other, and to fuse them to form a
bigger alignment, if this is the case.
As a side effect, all words we identify in this way are
strictly cognate, since our procedure does not allow to
identify a morpheme in the same language to be
cognate if this does not show the exact same form.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='fontsize:30px'>From alignments to strict, cross-semantic cognates</p>

<p style='text-align:center'>
<img src="img/cross-semantic-picture.png" style="width:800px" alt="img"></img>
</p>
<aside class="notes"><p>Here are some examples for the morphemes we found
in the data, which reoccurs in different words.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='fontsize:30px'>From alignments to strict, cross-semantic cognates</p>

<p style='text-align:center'>
<img src="img/cross-semantic-table.png" style="width:800px" alt="img"></img>
</p>

<aside class="notes"><p>Here we give another example to show you how
actually it is done. I would like to draw your attention
to the 東黔東 language. The tei²⁴ in Son and tei²⁴ in
Daughter were not in the same first analysis, in the
“cognacy” column, but now, after our analysis, in the
cross-semantic column, the algorithm found them to
be related, because the word is identical internally,
and our test with strict alignments accepted
this.</p>
</aside></script></section><section data-markdown><script type="text/template"><p stlye='fontsize:30px'>From alignments to strict, cross-semantic cognates</p>

<div class="spreadsheet" data-delimiter="\t" data-width="100" data-fontsize="12">
ID  \t DOCULECT        \t ENGLISH \t TOKENS             \t STRUCTURE \t ALIGNMENT \t CROSSIDS            \t COGIDS
 1   \t Baheng, east   \t SEVEN   \t tɕ a ³¹             \t i n t     \t tɕ a - ³¹ \t 3                  \t 3
 2   \t Baheng, west   \t SEVEN   \t tɕ a ŋ ⁴⁴           \t i n c t   \t tɕ a ŋ ⁴⁴ \t 3                  \t 3
 3   \t Qiandong, east \t SEVEN   \t ɕ u ŋ ⁵³            \t i n c t   \t  ɕ u ŋ ⁵³ \t 3                  \t 3
 4   \t Qiandong, wesst\t SEVEN   \t ɕ u ŋ ²²            \t i n c t   \t ɕ u ŋ ²²  \t 3                  \t 3
 5   \t Baheng, east   \t MOON    \t l a ³/⁰ + ɬ a ⁵⁵    \t i n t + i n t \t l a ³/⁰ + ɬ a ⁵⁵\t 1908 351 \t 1908 1907
 6   \t Baheng, west   \t MOON    \t ʔ a ³/⁰ + ɬ a ⁵⁵    \t i n t + i n t \t ʔ a ³/⁰ + ɬ a ⁵⁵ \t 41 351	\t 1909 1907
 7   \t Qiandong, east \t MOON    \t l a ⁴⁴ + l a ⁴⁴    \t i n t + i n t \t l a ⁴⁴ + l a ⁴⁴ \t 1908 351 \t 1908 1907
 8   \t Qiandong, wesst\t MOON    \t p ɔ ¹¹ + l a ³³    \t i n t + i n t \t p ɔ ¹¹ + l a ³³ \t 1910 351  \t 1910 1907
 9   \t Baheng, east   \t STAR    \t l a ³/⁰ + q a ŋ ³⁵ \t i n t + i n c t \t l a ³/⁰ + q a ŋ ³⁵ \t 1874 1834 \t 1874 1870
 10  \t Baheng, west   \t STAR    \t q a ³/⁰ + q a ŋ ³⁵ \t　i n t + i n c t \t q a ³/⁰ + q a ŋ ³⁵ \t 1872 1834 \t 1872 1870
 11  \t Qiandong, east \t STAR    \t q ei ²⁴ + q ei  ²⁴ \t　i n t + i n t \t q ei ²⁴ + q ei - ²⁴ \t  1872 1834 \t 1872 1870
 12  \t Qiandong, wesst\t STAR    \t t ei - ⁴⁴ + q ei ⁴⁴\t　i n t + i n t \t t ei - ⁴⁴ + q ei - ⁴⁴\t 1234 1834 \t 1871 1870
</div>
<aside class="notes"><p>And this is, how this all looks in our table. You cannot
see many differences here, but you can see that the
morphemes in MOON have been added to other sets
of morphemes, and we generally find a lot of crosssemantic
cognates in our data.</p>
</aside></script></section><section data-markdown><script type="text/template"><p style='text-align:center'>
<img src="img/strict-soundcorrespondence.png" style="width:800px" alt="img"></img>
</p>
<aside class="notes"><p>We can now start to search for sound correspondence
patterns.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>From strict cognates to sound correspondence patterns</h3>
<p style="text-align:center">
<img src="img/sound-correspondence-classic.png" alt="img" style="width:800px;text-align:center;"></img>
</p>

<aside class="notes"><p>Most linguists are probably familiar with this kind of
representation: Clackson shows, how sound
correspondence patterns are distributed over the Indo-
European languages.
But if we look at tables like this one, there are several
problems. <b>We don’t know the frequency of the
occurrence of each of the patterns, we do not know
the number of cognates in each individual
language, or the context, in which they occur. </b>
This makes it quite difficult to learn from text-books,
but also to criticize a theory and to try to improve it.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>From strict cognates to sound correspondence patterns</h3>
<p style="text-align:center">
<img src="img/ratliff2010.png" alt="img" style="width:600px;text-align:center;"></img>
</p>

<p style='text-align:center;font-size:60%;'>Ratliff et al. (2010). Hmong-Mien language history. Pacific Linguistics (Page 57)</p>

<aside class="notes"><p>A better example is provided by Martha Ratliff’s
study on Hmong-Mien in 2010. This table shows us
the cognate sets she used in the study, so in a way, it’s
more transparent than the previous example. But there
are remain some problems:
(1) the table does not provide the full words in the
examples, but only the morphemes
(2) the table does not provide us with information on
the degree to which patterns are inconsistent,
reflecting secondary variation in the individual
languages
(3) the representation is only to some degree machine-readable,
not only because it is not digitized, but also
because we do not know completely to which
correspondence pattern (or set) each of the other
sounds in the data belongs, and we will have problems
to check the consistency of entire words when given
the data in this form.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>From strict cognates to sound correspondence patterns</h3>
<p style="text-align:center">
<img src="img/edictor-hn.png" alt="img" style="width:600px;text-align:center;"></img>
</p>

<aside class="notes"><p>In our proposal for the handling of sound
correspondence patterns, we propose a different format, and specifically an interactive way of
inspecting patterns, along with an automatic analysis
by which the patterns can be inferred in a first place.
The result of this automatic analysis is like a table, in
which languages are placed in the columns, and
correspondence patterns are placed in rows, with each
cell indicating for each individual correspondence
pattern, which reflex sound a given language shows
for this pattern. In this format, everything can be
traced back to the original data, no data is “missing”
It’s also interactive, which I will demonstrate soon.</p>
</aside></script></section><section data-markdown><script type="text/template"><h3>Illustration of the Workflow</h3>

<p> Orthography profiles</p>
<a style="color:#2d1f23;text-align:center" href='http://calc.digling.org/profile/'> http://calc.digling.org/profile/</a></script></section><section data-markdown><script type="text/template"><h3>Illustration of the Workflow</h3>

<p>EDICTOR: a web-based tool to edit, analyse, and publish etymological data.</p>
<p style="text-align:center">
<a style="color:#2d1f23" href='http://edictor.digling.org/' align='middle'><img src='img/edictor.png' width=300px></img></a>
</p>
</script></section></section><section ><section data-markdown><script type="text/template">
<div class='modeling'></div>

<h3>Modeling and annotation</h3>
<p>Nathanael E. Schweikhard</p>
</script></section><section data-markdown><script type="text/template"><h3>Concepticon - unified concepts</h3>

<div class="fig-container" style="overflow:hidden;"
        data-overflow-shown=true
        data-file="https://concepticon.clld.org/"
        data-style="height: 620px; margin-top: -100px; width: 100%;"></script></section></section><section ><section data-markdown><script type="text/template"><div class='outlook'></div>
<img src='img/outlook.png'></img>
</script></section><section data-markdown><script type="text/template"><ul>
  <li>Increasing the accuracy</li>
  <li>More user-friendly</li>
</ul>
</script></section></section><section  data-markdown><script type="text/template"><div></div>
<h3>Thank you for the attention!</h3>
</script></section></div>

      <footer id="footer">
        <p>Tiago Tresoldi | Mei-Shin Wu | Nathanael E. Schweikhard</p>
      </footer>
    </div>

    <script src="./lib/js/head.min.js"></script>
    <script src="./js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }
      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: './plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './plugin/zoom-js/zoom.js', async: true },
        { src: './plugin/notes/notes.js', async: true },
        { src: './plugin/math/math.js', async: true },
        { src: './plugin/reveald3/reveald3.js' },
        { src: './plugin/spreadsheet/spreadsheet.js'}
      ];
      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps
      };
      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};
      var options = extend(defaultOptions, {}, queryOptions);
    </script>

    <script src="./_assets/jquery.js"></script>
    <script src="./_assets/additional.js"></script>
    <script src="./_assets/ruleJS.all.full.min.js"></script>

    <script>
      Reveal.initialize(options);
    </script>

  </body>

</html>
